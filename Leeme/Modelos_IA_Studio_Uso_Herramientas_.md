

# **Capacidades de Llamada a Funciones en Modelos de Google AI Studio**

## **I. Resumen Ejecutivo**

Google AI Studio ofrece un entorno robusto para aprovechar la llamada a funciones (también conocida como uso de herramientas) con sus familias de modelos Gemini y Gemma. Esta capacidad permite a los Modelos de Lenguaje Grandes (LLM) interactuar con sistemas externos, recuperar datos en tiempo real, realizar cálculos y automatizar acciones, extendiendo así su utilidad más allá de la mera generación de texto. La plataforma agiliza el acceso a estas características avanzadas, haciéndolas accesibles para los desarrolladores.

Entre las principales fortalezas, los modelos de Google, particularmente Gemini 2.0 Flash y 2.5 Pro/Flash, ofrecen una comprensión multimodal nativa, ventanas de contexto excepcionalmente largas (hasta 1 millón de tokens), y soporte sofisticado para llamadas a funciones paralelas y composicionales.1 La serie Gemma 3 también soporta la llamada a funciones, proporcionando una alternativa ligera y de código abierto adecuada para flujos de trabajo agenciales, incluso en entornos con recursos limitados o dispositivos de borde.7 Además, la plataforma Google AI Studio en sí misma ofrece un nivel completamente gratuito para experimentación, lo que reduce significativamente la barrera de entrada para los desarrolladores.11

Sin embargo, la implementación de la llamada a funciones requiere que la aplicación del desarrollador gestione la ejecución real de la herramienta y devuelva los resultados al modelo.12 La retroalimentación de la comunidad destaca inconsistencias y desafíos ocasionales, como modelos que generan fragmentos de código en lugar de las llamadas a funciones deseadas en conversaciones de múltiples turnos, o que exhiben un "comportamiento incoherente" en escenarios altamente complejos.14 Adicionalmente, los límites de tasa varían según el modelo y el nivel de uso, lo que requiere una planificación cuidadosa para despliegues de producción escalables.17

La filosofía de diseño explícita de Google para sus modelos subraya un fuerte compromiso con la IA agencial. Los modelos Gemini están específicamente optimizados para "experiencias agenciales" 18, mientras que los modelos Gemma están construidos con "capacidades agenciales" en mente.7 Esto no es simplemente una característica, sino un aspecto fundamental de su arquitectura. El énfasis constante en la entrada multimodal 4 y las ventanas de contexto expansivas 4 apoyan directamente el desarrollo de flujos de trabajo agenciales complejos. Estos flujos de trabajo a menudo exigen la capacidad de procesar entradas diversas y grandes (por ejemplo, repositorios de código completos, documentos largos, video) y mantener una memoria conversacional extensa para un razonamiento coherente y de múltiples pasos. La característica de "presupuesto de pensamiento" disponible con Gemini Flash 5 ejemplifica aún más esto, permitiendo un control granular sobre el proceso de inferencia del modelo para equilibrar el rendimiento y el costo. Este enfoque colectivo sugiere que Google está invirtiendo fuertemente en permitir agentes de IA sofisticados que no solo puedan comprender, sino también interactuar y manipular activamente el mundo real a través de herramientas, yendo más allá de las simples interfaces conversacionales. Esta posición estratégica convierte a Google AI Studio en una opción atractiva para desarrolladores y organizaciones que buscan construir aplicaciones avanzadas de IA que requieran interacción externa, automatización y procesamiento de datos en tiempo real. Esto significa un futuro donde los modelos de IA de Google serán parte integral de la automatización y mejora de las operaciones del mundo real.

## **II. Comprensión de la Llamada a Funciones en LLMs**

La llamada a funciones, a menudo referida indistintamente como llamada a herramientas, es una capacidad fundamental que otorga a los LLM acceso a herramientas y API externas. Este acceso permite a los modelos realizar acciones o recuperar información que va más allá de sus datos de entrenamiento inherentes o de su fecha de corte de conocimiento actual.18

Es crucial entender que el propio LLM no ejecuta directamente la herramienta externa. En su lugar, analiza inteligentemente la solicitud del usuario y, si se considera necesaria una herramienta externa para satisfacer la petición, sugiere la herramienta a llamar. Esta sugerencia se proporciona en un formato estructurado y legible por máquina, típicamente un objeto JSON, que incluye el nombre de la función y los parámetros necesarios extraídos de la entrada del usuario.18 La responsabilidad de ejecutar la herramienta sugerida recae en la aplicación del usuario o en el marco de integración. Una vez que la herramienta ha sido ejecutada y se han obtenido los resultados, estos se devuelven al LLM. El LLM procesa entonces estos resultados para formular una respuesta final, contextualmente relevante y amigable para el usuario.18 Este modelo de interacción de múltiples pasos facilita un "pensamiento intercalado", permitiendo que el modelo considere los resultados de una llamada a herramienta antes de decidir sobre acciones subsiguientes o formular una respuesta final.18

Las principales aplicaciones de la llamada a funciones identificadas por Google incluyen:

* **Aumentar el conocimiento:** La llamada a funciones es fundamental para permitir que los LLM accedan y recuperen información actualizada o propietaria de fuentes externas como bases de datos, API de terceros (por ejemplo, servicios meteorológicos, precios de acciones) y bases de conocimiento internas.12 Esto supera la limitación inherente de los datos de entrenamiento estáticos de un LLM.  
* **Ampliar capacidades:** Permite a los LLM realizar cálculos o tareas que están fuera de sus habilidades básicas de generación de lenguaje. Ejemplos incluyen el uso de una calculadora para operaciones matemáticas precisas, la generación de gráficos a partir de datos o el procesamiento de estructuras de datos complejas.12  
* **Realizar acciones:** Quizás el caso de uso más potente, la llamada a funciones permite a los LLM interactuar y controlar sistemas externos utilizando API. Esto facilita la automatización del mundo real, como la programación de citas, la generación y envío de facturas, el envío de correos electrónicos o el control de dispositivos domésticos inteligentes.12

La afirmación recurrente de que "el modelo no ejecuta la función por sí mismo" 18 es una decisión arquitectónica profunda. Significa que el LLM funciona principalmente como un orquestador inteligente o un "cerebro", capaz de comprender la intención y decidir

*qué* acción externa se necesita y *cómo* parametrizarla. Sin embargo, la ejecución real del código, la interacción con las API externas y el manejo de datos del mundo real se delega explícitamente a la aplicación del desarrollador. Esta elección de diseño ofrece varias ventajas: descarga la carga computacional y los posibles riesgos de seguridad de la ejecución arbitraria de código del propio LLM; permite a los desarrolladores integrar herramientas altamente específicas, de alto rendimiento y seguras adaptadas a sus necesidades únicas sin requerir el reentrenamiento del modelo base; y asegura que las operaciones sensibles permanezcan dentro del control y el perímetro de seguridad de la aplicación. En consecuencia, el éxito y la fiabilidad generales de un sistema de IA que utiliza la llamada a funciones dependen en gran medida de la robustez, seguridad y eficiencia de las implementaciones de herramientas externas del desarrollador y de la eficacia del bucle de retroalimentación que devuelve los resultados al LLM. Este patrón arquitectónico promueve la modularidad y la extensibilidad en el desarrollo de aplicaciones de IA. Desplaza la complejidad de construir modelos de IA monolíticos y que lo abarcan todo, hacia el diseño de ecosistemas de herramientas eficientes, seguros y fiables que aumentan las capacidades de los LLM. Para los desarrolladores, esto significa invertir en patrones sólidos de integración de API, manejo integral de errores y gestión robusta del estado dentro de sus aplicaciones. También implica que, si bien los LLM son cada vez más inteligentes, siguen siendo herramientas que requieren una integración y gestión cuidadosas dentro de un sistema de software más amplio.

### **Tabla 2: Pasos del Flujo de Trabajo de Llamada a Funciones (Google AI)**

Este modelo de interacción de múltiples pasos es fundamental para la implementación práctica y la depuración. La tabla siguiente desglosa el proceso complejo en etapas digeribles, facilitando la comprensión del flujo de interacción entre la aplicación y el LLM, y delineando claramente las responsabilidades en cada paso.

| Número de Paso | Descripción del Paso | Actor (LLM/Aplicación) | Acción/Salida Clave |
| :---- | :---- | :---- | :---- |
| 1 | Definir la Declaración de Función | Aplicación | Esquema estructurado (JSON/Python) que describe el nombre, propósito y parámetros de la herramienta. |
| 2 | Llamar al LLM con Declaraciones | Aplicación | Solicitud del usuario \+ Declaración(es) de Función enviada(s) al LLM. |
| 3 | El LLM Responde con la Llamada a Función | LLM | Objeto JSON estructurado que contiene el nombre de la función y los argumentos (ej., {"name": "get\_weather", "args": {"location": "London"}}). |
| 4 | Ejecutar el Código de la Función | Aplicación | Analizar la respuesta del LLM, ejecutar la función externa correspondiente con los argumentos proporcionados. |
| 5 | Enviar los Resultados de la Función al LLM | Aplicación | La salida de la función (ej., {"temperature": 16, "unit": "Celsius"}) se envía de vuelta al LLM como una function\_response. |
| 6 | El LLM Proporciona la Respuesta Final | LLM | Respuesta en lenguaje natural que incorpora los resultados de la función, presentada al usuario. |

## **III. Modelos de Google AI Studio con Soporte para Llamada a Funciones**

Google AI Studio y la API de Gemini subyacente facilitan la llamada a funciones en una gama de modelos dentro de las familias Gemini y Gemma.1 Estos modelos están estratégicamente diseñados para diversos casos de uso, ofreciendo diferentes equilibrios de inteligencia, velocidad y rentabilidad.5

### **Serie Gemini**

* **Gemini 2.5 Pro:** Posicionado como el modelo más inteligente de Google para abordar tareas complejas, Gemini 2.5 Pro soporta una amplia gama de entradas multimodales, incluyendo texto, código, imágenes, audio y video, junto con sólidas capacidades de llamada a funciones. Cuenta con una impresionante ventana de contexto de entrada de 1.048.576 tokens (1M de tokens).1 Este modelo destaca particularmente en tareas avanzadas de razonamiento y codificación.20  
* **Gemini 2.5 Flash:** Descrito como un "modelo potente y eficiente diseñado para la velocidad y el bajo costo", Gemini 2.5 Flash es ideal para tareas como la resumir, aplicaciones de chat, extracción de datos y subtitulado. Al igual que su contraparte Pro, soporta entrada multimodal y una sustancial ventana de contexto de 1 millón de tokens.1 Una característica clave es su inferencia dinámica, que permite "presupuestos de pensamiento" ajustables para optimizar la latencia y el costo.5  
* **Gemini 2.0 Flash (incluyendo versiones Experimental/Lite):** Esta iteración ofrece un Tiempo hasta el Primer Token (TTFT) significativamente más rápido en comparación con su predecesor, Gemini 1.5 Flash, al tiempo que introduce mejoras notables en la comprensión multimodal, capacidades de codificación, seguimiento de instrucciones complejas y llamada a funciones.18 También soporta una gran ventana de contexto de entrada de 1.048.576 tokens.6 La variante  
  gemini-2.0-flash-lite se destaca específicamente como generalmente disponible para uso en producción, enfatizando su preparación para aplicaciones del mundo real.23 Todos los modelos Gemini 2.5 y 2.0 Flash/Pro mencionados están explícitamente listados como compatibles con la llamada a funciones dentro de la documentación de la API de Llamada a Funciones.1

### **Serie Gemma**

* **Gemma 3 (1B, 4B, 12B, 27B):** Esta familia representa modelos ligeros y de código abierto de vanguardia desarrollados por Google, construidos sobre la misma investigación y tecnología fundamental utilizada para los modelos Gemini.21 Los modelos Gemma 3 (específicamente los tamaños 4B, 12B y 27B) soportan una ventana de contexto de 128K tokens, mientras que la versión más pequeña de 1B cuenta con un contexto de 32K tokens.19 Son multimodales, capaces de manejar tanto texto como entrada de imagen para generar salida de texto, con la excepción del modelo 1B que es solo texto.19 La llamada a funciones es compatible con los modelos Gemma, aunque generalmente requiere una elaboración cuidadosa de las instrucciones para guiar al modelo a generar la llamada a funciones en un formato estructurado.22  
* **Gemma 3n (2B, 4B):** Estos modelos están optimizados para una implementación eficiente en entornos de bajos recursos. Generalmente ofrecen una longitud de contexto de 32K (aunque una variante específica, Gemma 3n E2B IT, se observa con un contexto de 8K en 25) y demuestran un fuerte rendimiento multilingüe y de inferencia en benchmarks comunes.25 También soportan la llamada a funciones, lo que los hace adecuados para aplicaciones agenciales donde los recursos computacionales son una limitación.27

La provisión por parte de Google de una gama diversa de modelos (desde el altamente inteligente Gemini Pro, hasta el Gemini Flash optimizado para costo y velocidad, y la serie Gemma ligera y de código abierto), todos equipados con capacidades de llamada a funciones, revela una estrategia de producto deliberada y sofisticada. Este enfoque tiene como objetivo satisfacer un amplio espectro de necesidades de desarrolladores y requisitos de aplicaciones, que van desde sistemas agenciales complejos y de alto rendimiento (donde Gemini Pro sobresale) hasta aplicaciones eficientes y sensibles al costo (adecuadas para Gemini Flash) e incluso implementaciones en dispositivos o con recursos limitados (donde Gemma destaca). El etiquetado frecuente de modelos como "Experimental" o "Preview" 18 indica una estrategia de iteración rápida y lanzamiento continuo. Esto anima a los desarrolladores a experimentar temprano con características de vanguardia, aunque con el entendimiento de que estas versiones pueden ser menos estables que los modelos generalmente disponibles. Esta oferta escalonada capacita a los desarrolladores para tomar decisiones altamente informadas al seleccionar un LLM, permitiéndoles equilibrar con precisión la inteligencia, la velocidad, el costo y el entorno de implementación específico. Subraya la idea de que no existe un LLM "único para todos" y que el rendimiento óptimo se logra al hacer coincidir el modelo con la tarea. Además, el ciclo de lanzamiento rápido implica que el "estado del arte" en IA es un objetivo en constante movimiento, lo que requiere que los desarrolladores se mantengan ágiles, evalúen continuamente los nuevos lanzamientos y adapten sus estrategias de integración para aprovechar los últimos avances y navegar por las capacidades y limitaciones en evolución.

### **Tabla 1: Modelos de Google AI Studio que Soportan Llamada a Funciones**

La siguiente tabla ofrece una visión general comparativa de los modelos clave de Google que soportan la llamada a funciones, detallando sus características principales y casos de uso típicos. Este formato conciso ayuda a los desarrolladores a identificar rápidamente los modelos adecuados para sus requisitos de proyecto específicos.

| Nombre del Modelo | Ventana de Contexto (Entrada) | Soporte Multimodal | Caso de Uso/Optimización Principal | Soporte Llamada a Funciones | Estado de Disponibilidad | Nivel de Costo (en OpenRouter/Google AI Studio) |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| Gemini 2.5 Pro | 1.048.576 tokens (1M) | Texto, Código, Imágenes, Audio, Video | Tareas complejas, inferencia avanzada, codificación | Sí | GA (Generalmente Disponible) | Pago |
| Gemini 2.5 Flash | 1.000.000 tokens (1M) | Texto, Audio, Imágenes, Video | Velocidad, bajo costo, resumen, chat, extracción de datos | Sí | GA (Generalmente Disponible) | Pago |
| Gemini 2.0 Flash | 1.048.576 tokens (1M) | Texto, Código, Imágenes, Audio, Video | TTFT más rápido, experiencias agenciales | Sí | GA (Generalmente Disponible) | Pago (Experimental Gratuito en OpenRouter) |
| Gemma 3 (4B, 12B, 27B) | 128.000 tokens (128K) | Texto, Imagen | Ligero, código abierto, generación general, inferencia | Sí (vía instrucciones) | GA (Pesos Abiertos) | Gratuito (AI Studio), Gratuito/Pago (OpenRouter) |
| Gemma 3 (1B) | 32.000 tokens (32K) | Solo Texto | Ligero, código abierto, generación general, inferencia | Sí (vía instrucciones) | GA (Pesos Abiertos) | Gratuito (AI Studio), Gratuito/Pago (OpenRouter) |
| Gemma 3n (2B, 4B) | 32.000 tokens (32K) (8K para E2B IT) | Multimodal (Texto, Imagen, Video, Audio) | Despliegue en entornos de bajos recursos, ejecución eficiente | Sí | GA (Pesos Abiertos) | Gratuito (OpenRouter) |

## **IV. Implementación de Llamada a Funciones con Modelos de Google AI**

### **Definición de Declaraciones de Función**

Para habilitar la llamada a funciones, los desarrolladores deben definir objetos FunctionDeclaration. Estas declaraciones informan al LLM sobre las herramientas externas disponibles. Cada declaración incluye un name para la función, una description que describe su propósito, y un campo parameters que especifica las entradas esperadas.12

Los parameters se describen utilizando el formato de Objeto JSON Schema de OpenAPI, lo que permite una definición precisa del type de datos (por ejemplo, STRING, INTEGER, BOOLEAN, NUMBER, ARRAY, OBJECT), description, enum (valores posibles), items (para arrays), properties (para objetos) y campos required.1 Esta estandarización asegura una comunicación clara entre el modelo y la aplicación. Los modelos Google Gemini ofrecen flexibilidad en la definición de funciones: pueden definirse utilizando directamente el JSON Schema compatible con OpenAPI, o, para desarrolladores Python, el SDK puede generar automáticamente el esquema a partir de definiciones de funciones Python bien estructuradas con docstrings.2

Para los modelos Gemma, el enfoque es ligeramente diferente: no generan un token específico de herramienta. En su lugar, la llamada a funciones se logra mediante instrucciones cuidadosas que le indican explícitamente al modelo el formato de salida estructurado deseado (por ejemplo, llamadas de estilo JSON o Python). El marco del desarrollador debe entonces detectar y analizar esta salida estructurada para identificar y ejecutar la llamada a la herramienta.22

### **El Flujo de Interacción de Múltiples Pasos (Detallado)**

El proceso de llamada a funciones es una conversación sofisticada y de múltiples turnos entre el usuario, el LLM y las herramientas externas. A continuación, se detalla el flujo de interacción:

1. **Definir la Declaración de Función:** El código de la aplicación del desarrollador define el nombre de la función, su propósito y un esquema detallado para sus parámetros. Esta declaración se pone a disposición del LLM.  
2. **Llamar al LLM con Declaraciones:** La solicitud del usuario, junto con las declaraciones de función definidas, se envía al LLM. El modelo analiza la solicitud del usuario y las herramientas disponibles para determinar si una llamada a función sería beneficiosa o necesaria para cumplir la consulta.  
3. **El LLM Responde con la Llamada a Función:** Si el LLM decide utilizar una herramienta, genera un objeto JSON estructurado. Este objeto especifica el name de la función a llamar y los args (argumentos) extraídos de la entrada en lenguaje natural del usuario.18 La respuesta de texto directo del modelo podría ser nula en este caso.  
4. **Ejecutar el Código de la Función (Responsabilidad del Cliente):** La aplicación recibe la respuesta del LLM. Luego analiza el objeto functionCall, extrae el nombre de la función y los argumentos, y ejecuta la función externa correspondiente dentro del entorno de la aplicación.18 Este paso es completamente gestionado por el código del desarrollador.  
5. **Enviar los Resultados de la Función al LLM:** Una vez que la función externa ha completado su ejecución, se capturan sus resultados. Estos resultados se envían de vuelta al LLM como una function\_response en un turno subsiguiente de la conversación, proporcionando al modelo la información necesaria para completar la tarea.18  
6. **El LLM Proporciona la Respuesta Final:** El LLM procesa los resultados de la función recibidos, los integra con el contexto de la conversación en curso y genera una respuesta final, coherente y amigable para el usuario en lenguaje natural a la consulta original del usuario.18 Esto completa el ciclo de interacción.

### **Manejo de Llamadas a Funciones Únicas, Paralelas y Composicionales**

Una fortaleza significativa de los modelos Gemini de Google es su capacidad para manejar escenarios complejos de uso de herramientas. Soportan la llamada a múltiples funciones en un solo turno (conocida como llamada a funciones paralela) cuando la consulta de un usuario implica la necesidad de recuperación o acciones concurrentes de información.12 Además, soportan la llamada a funciones composicional, donde el modelo puede encadenar múltiples llamadas a herramientas en secuencia, con pasos de inferencia intermedios, para resolver problemas más complejos y de múltiples pasos.18 Esto permite una toma de decisiones más matizada basada en resultados intermedios.

### **Integración con API y Marcos Externos**

Los modelos de Google están diseñados para integrarse en ecosistemas de aplicaciones de IA más amplios. Pueden integrarse sin problemas con marcos populares como LangChain, que proporciona abstracciones y herramientas para construir aplicaciones impulsadas por LLM. create\_tool\_calling\_agent y AgentExecutor de LangChain pueden automatizar el proceso de comprender cuándo usar herramientas, generar argumentos y procesar las salidas de las herramientas.2 Para los desarrolladores que ya trabajan con la API de OpenAI, Google proporciona un punto final de API compatible con OpenAI. Esto permite aprovechar el código y los SDK existentes basados en OpenAI para la llamada a funciones con modelos Gemini, simplificando la migración o los despliegues de múltiples modelos.2

Si bien los LLM de Google demuestran capacidades impresionantes al *sugerir* llamadas a funciones y extraer argumentos, la responsabilidad crítica de *ejecutar* esas llamadas, manejar sus resultados y gestionar la intrincada conversación de múltiples turnos (especialmente con llamadas paralelas y composicionales) recae completamente en la aplicación del desarrollador.18 Esto implica que una implementación robusta de llamada a funciones es mucho más que simplemente realizar una llamada a la API al LLM. Requiere una capa de orquestación sofisticada dentro de la aplicación del desarrollador que debe incluir: un manejo robusto de errores para las llamadas a API externas, una gestión efectiva del estado para mantener el contexto conversacional en múltiples turnos y, potencialmente, lógica de reintento para fallas transitorias del servicio externo. El concepto de "pensamiento intercalado" 18 enfatiza aún más que el flujo de interacción no es una simple solicitud-respuesta, sino un ida y vuelta dinámico entre la inferencia del LLM y las interacciones del mundo real de la aplicación. Este modelo arquitectónico significa que los desarrolladores deben diseñar sus sistemas de IA con este paradigma de "aplicación en el bucle" firmemente en mente. El éxito depende de invertir en patrones sólidos de integración de API, implementar un manejo integral de errores y, potencialmente, adoptar marcos agenciales (como LangChain) para simplificar la compleja orquestación requerida. Destaca que, si bien los LLM son potentes, son componentes dentro de un sistema más grande, y su efectividad está intrínsecamente ligada a la calidad y robustez de la lógica de aplicación circundante.

## **V. Rendimiento, Fiabilidad y Consideraciones Prácticas**

### **Indicadores Clave de Rendimiento (KPIs)**

* **Latencia y Rendimiento (Throughput):** Los modelos Gemini Flash están optimizados para la velocidad. Gemini 2.0 Flash ofrece un Tiempo hasta el Primer Token (TTFT) significativamente más rápido en comparación con su predecesor, Gemini 1.5.18 La latencia para Gemini 2.0 Flash Experimental se reporta entre 0.70s y 1.32s, con rendimientos que van de 124.1 a 143.1 tokens por segundo (tps).18 Gemma 3n E2B IT también muestra una baja latencia de 0.60s y 50.49 tps.25  
* **Precisión:** Gemini 2.5 Pro demuestra un fuerte rendimiento en benchmarks de inferencia y codificación, logrando una puntuación del 63.8% en SWE-Bench Verified con una configuración de agente personalizada.20 Aunque no se detallan explícitamente benchmarks directos y completos para la precisión de la llamada a funciones de múltiples turnos de Gemini en los fragmentos proporcionados, la retroalimentación de la comunidad (discutida a continuación) sugiere que el rendimiento en el mundo real a veces puede desviarse del ideal. A modo de comparación, un competidor como DeepSeek R1-0528 muestra una alta precisión en la llamada a funciones de un solo turno, pero lucha significativamente con escenarios de múltiples turnos, con una precisión tan baja como el 4-6%.29  
* **Impacto de la Ventana de Contexto:** Los modelos Gemini de Google presentan ventanas de contexto muy grandes: Gemini 2.5 Pro soporta 1.048.576 tokens (1M), con 2M de tokens "próximamente", y Gemini 2.0 Flash también soporta 1M de tokens.4 Los modelos Gemma 3 (4B, 12B, 27B) ofrecen un contexto de 128K tokens, mientras que la versión 1B tiene 32K.19 Estas grandes ventanas de contexto están diseñadas para permitir el procesamiento de vastos conjuntos de datos y el manejo de problemas complejos y multifacéticos.9 Sin embargo, los informes de la comunidad indican que, a pesar de estas grandes ventanas, los modelos Gemini pueden volverse "casi inutilizables" alrededor del 80-85% de la capacidad de la ventana de contexto, lo que lleva a problemas como solicitudes ignoradas, respuestas a solicitudes antiguas y respuestas repetitivas.30

### **Problemas Comunes y Retroalimentación de la Comunidad**

* **Fiabilidad y Predictibilidad:** Los usuarios han informado que la llamada a funciones de Gemini 2.5 puede ser "completamente poco fiable e impredecible". Esto incluye casos en los que el modelo genera texto plano en lugar de la llamada a funciones esperada, o, cuando sí llama a funciones, la calidad de la salida es a veces peor que una respuesta de texto directo.14  
* **Bloques de Código Inesperados:** Un "error de larga data" implica que el modelo inicie su respuesta con un bloque de código markdown json, incluso cuando se le ha instruido explícitamente que no lo haga. Esto obliga a los desarrolladores a implementar soluciones alternativas.14 Se observa un problema similar con Gemini 2.0 Flash en llamadas a herramientas de múltiples pasos, donde genera código Python markdown en lugar de activar la ejecución real de la herramienta.16  
* **Manejo del Contexto en Conversaciones de Múltiples Turnos:** Los problemas persistentes incluyen que los modelos pierdan el enfoque en la solicitud más reciente del usuario, respondan a solicitudes enviadas mucho antes, o se vuelvan "increíblemente tontos" en conversaciones complejas de múltiples turnos, particularmente a medida que el tamaño del contexto crece más allá de los 2K tokens.14  
* **Error de "Browsing":** Una peculiaridad específica observada es la incapacidad de Gemini Flash 2.5 para usar la palabra "browsing" en su propia interfaz web, capitalizándola consistentemente como "Browse" debido a una función de herramienta interna. Si bien esto es menos problemático al interactuar a través de la API, destaca los sesgos internos del modelo.31  
* **Sobre-ingeniería/Alucinación en la Codificación:** En tareas de codificación, se informó que Gemini 2.5 Flash "arreglaba" las clases de prueba haciéndolas devolver true, una solución "descerebrada".15 Gemini 2.5 Pro ha sido criticado por sobre-ingeniería de soluciones o "alucinación" de cambios de código que se extienden más allá del alcance solicitado.32  
* **Errores de "Modelo Inválido":** Los usuarios han encontrado errores de INVALID MODEL al intentar usar gemini-2.5-pro-latest, lo que requiere una solución temporal de cambiar a versiones de vista previa como gemini-2.5-pro-preview-06-05.32

### **Mejores Prácticas para una Implementación Robusta de Llamada a Funciones (según Google)**

* **Claridad y Detalle:** Proporcionar nombres de funciones claros y detallados, descripciones completas de los parámetros e instrucciones explícitas dentro de las declaraciones de función.34  
* **Parámetros Fuertemente Tipados:** Utilizar tipado fuerte para los parámetros para asegurar que el modelo genere argumentos en el formato esperado, reduciendo errores de análisis.34  
* **Instrucciones del Sistema Efectivas:** Aprovechar las instrucciones del sistema para guiar el comportamiento general del modelo y reforzar los patrones deseados de llamada a funciones.34  
* **Actualizaciones de la Solicitud (Prompt):** Actualizar continuamente la solicitud del usuario con contexto relevante para ayudar al modelo a mantener el enfoque y tomar decisiones precisas.34  
* **Configuración de Generación:** Utilizar parámetros dentro de FunctionCallingConfig como mode (AUTO, NONE, ANY) y allowed\_function\_names para controlar con precisión cuándo y qué funciones puede llamar el modelo.1 El modo  
  ANY fuerza una llamada a función.  
* **Validar y Proteger:** Siempre validar la llamada a la API y, crucialmente, implementar salvaguardias robustas para validar cualquier código o argumento de función generado antes de la ejecución para prevenir acciones no deseadas o maliciosas.22

Si bien los modelos de Google demuestran un rendimiento impresionante en los benchmarks académicos, particularmente en la inferencia y la codificación 7, la retroalimentación colectiva de la comunidad revela una disparidad significativa en cuanto a la fiabilidad y consistencia en el mundo real, especialmente en escenarios complejos de llamada a funciones de múltiples turnos.14 Esto sugiere que las metodologías de benchmarking actuales pueden no capturar completamente los desafíos matizados de los flujos de trabajo agenciales dinámmicos e iterativos, particularmente aquellos que involucran un contexto conversacional largo y el uso repetido de herramientas. La observación de que un "modelo de vista previa es mejor que el de producción" 14 es particularmente preocupante, ya que apunta a posibles problemas en los procesos internos de lanzamiento y garantía de calidad de Google, lo que lleva a la frustración de los desarrolladores y a la falta de confianza en los modelos listos para producción. Los problemas reportados de sobre-ingeniería, alucinación y desviación de las instrucciones 15 indican además una lucha por mantener una adherencia estricta a las restricciones definidas por el desarrollador en tareas de codificación complejas, lo cual es primordial para sistemas agenciales fiables. Los desarrolladores que planean integrar las capacidades de llamada a funciones de Google en sistemas de producción deben adoptar un enfoque altamente cauteloso y proactivo. Esto incluye implementar un manejo de errores extenso y robusto, un monitoreo integral de las salidas del LLM y el diseño de mecanismos de respaldo para comportamientos inesperados del modelo. Confiar únicamente en las puntuaciones de los benchmarks publicados para aplicaciones agenciales puede ser engañoso; las pruebas de estrés en el mundo real son esenciales. La retroalimentación de la comunidad subraya la necesidad crítica de una mejora continua en la consistencia del modelo, el seguimiento de instrucciones y la gestión del contexto, especialmente a medida que los modelos se escalan para manejar contextos más grandes e interacciones más complejas. También destaca la importancia de un registro y depuración detallados dentro de la aplicación del desarrollador para diagnosticar y comprender el comportamiento del LLM.

### **Tabla 3: Retos Comunes y Mitigación de la Llamada a Funciones (Modelos de Google AI)**

Esta tabla aborda directamente los puntos problemáticos reportados por la comunidad de desarrolladores, proporcionando una visión estructurada de los problemas conocidos y estrategias accionables para que los desarrolladores los mitiguen.

| Reto/Problema | Descripción | Modelos Afectados (si se especifica) | Estrategia de Mitigación/Mejor Práctica | IDs de Fragmentos Relevantes |
| :---- | :---- | :---- | :---- | :---- |
| Llamadas a Funciones Poco Fiables/Impredecibles | El modelo a veces genera texto plano en lugar de la llamada a función, o proporciona una salida deficiente al llamar. | Gemini 2.5 (Flash/Pro) | Mejorar las descripciones de las herramientas y las instrucciones del prompt; usar mode='ANY' en FunctionCallingConfig para forzar llamadas; implementar manejo de errores robusto y mecanismos de respaldo. | 14 |
| Bloques de Código Markdown Inesperados | El modelo genera código markdown json o Python en lugar de ejecutar la herramienta; empeora con el contexto. | Gemini 2.0 Flash, Gemini 2.5 | Implementar una capa de análisis del lado del cliente para extraer y ejecutar bloques de código si el modelo no llama directamente a la herramienta; refinar las instrucciones para desincentivar la generación de código. | 14 |
| Degradación de la Ventana de Contexto | El rendimiento se degrada, el modelo ignora las instrucciones, repite respuestas o responde a consultas antiguas con una carga de contexto alta (80-85%). | Gemini 2.5 Pro | Monitorear el tamaño de la ventana de contexto; iniciar nuevos chats para tareas complejas; usar NotebookLM para documentos largos; dividir PDFs/datos grandes e indexarlos para referencia en lugar de mantenerlos en la memoria activa. | 30 |
| Sobre-ingeniería/Alucinación (Codificación) | El modelo modifica incorrectamente las clases de prueba, se excede en el alcance o alucina cambios de código. | Gemini 2.5 Flash, Gemini 2.5 Pro | Proporcionar instrucciones extremadamente directas y específicas; implementar una validación estricta del código generado; usar marcos agenciales con límites claros. | 15 |
| Errores de "Modelo Inválido" | Error al usar gemini-2.5-pro-latest. | Gemini 2.5 Pro | Usar IDs de modelos de vista previa específicos (ej., gemini-2.5-pro-preview-06-05) como solución temporal; mantenerse actualizado sobre los cambios de nombre de los modelos. | 32 |
| Error de Capitalización de "Browsing" | El modelo capitaliza "Browse" en lugar de "browsing" en lenguaje natural. | Gemini Flash 2.5 | Principalmente un problema de UI/herramienta interna; menos impactante a través de la API; no se necesita mitigación directa para los usuarios de la API. | 31 |

## **VI. Precios y Niveles de Uso en Google AI Studio/API**

### **Google AI Studio: Completamente Gratuito para su Uso**

Una ventaja significativa para los desarrolladores es que "el uso de Google AI Studio es completamente gratuito en todos los países disponibles".11 Esta política fomenta activamente la creación de prototipos, la experimentación y el aprendizaje sin barreras de costos inmediatas.

### **Nivel Gratuito de la API de Gemini**

La API de Gemini ofrece un "nivel gratuito" diseñado específicamente para fines de prueba y desarrollo, aunque con límites de tasa más bajos en comparación con los niveles de pago.11 Bajo este nivel gratuito, los precios de entrada y salida son "gratuitos" para una amplia gama de modelos, incluyendo varias versiones de Gemini 2.5 Flash, Gemini 2.5 Pro, Gemini 2.0 Flash, Gemma y Embeddings.11 Esto lo hace altamente accesible para la integración y validación inicial. El Grounding con Google Search, una característica que mejora el conocimiento del modelo, también está disponible de forma gratuita hasta 500 solicitudes por día (RPD).11 Es importante tener en cuenta que "los modelos de vista previa pueden cambiar antes de volverse estables y tienen límites de tasa más restrictivos".11 Una consideración clave para el nivel gratuito es que los datos (solicitudes y finalizaciones)

*pueden ser utilizados por Google para mejorar sus productos*.11 Este es un compromiso común para el acceso gratuito.

### **Niveles de Pago de la API de Gemini**

Para aplicaciones que requieren mayor escala, rendimiento y características dedicadas, Google ofrece niveles de pago con límites de tasa significativamente aumentados.11 Los precios suelen estructurarse por 1 millón (M) de tokens, con tarifas distintas para entrada y salida. Estas tarifas varían según el modelo específico y el tamaño del contexto de la solicitud.11 Por ejemplo, Gemini 2.5 Flash tiene un precio de $0.10 por 1M de tokens de entrada (para texto, imagen y video) y $0.40 por 1M de tokens de salida.11 Las características adicionales, como el almacenamiento en caché del contexto, están disponibles exclusivamente en los niveles de pago e incurren en cargos separados.11 Los modelos especializados, como los de interacciones de API en vivo o generación de video, también tienen sus propias estructuras de precios de pago específicas.11 La progresión a niveles de uso más altos (Nivel 1, Nivel 2, Nivel 3\) se basa en el gasto acumulado total en los servicios de Google Cloud asociados con la cuenta de facturación vinculada al proyecto.17 Esto incentiva una adopción más amplia de Google Cloud.

### **Suscripciones a Google AI Pro/Ultra (Orientadas al Consumidor)**

Es importante distinguir entre la API de Gemini, orientada a desarrolladores, y las suscripciones de IA de Google, orientadas al consumidor. Google AI Pro ($19.99/mes, con el primer mes gratis) y Google AI Ultra ($249.99/mes, con una tarifa con descuento durante los primeros tres meses) ofrecen acceso premium a la aplicación Gemini, Veo (generación de video), Flow (creación de películas con IA), Whisk (imagen a video) y NotebookLM, junto con límites de uso y almacenamiento aumentados.13 Estas suscripciones representan una estrategia de monetización directa al consumidor, separada de la API para desarrolladores.

### **Límites de Tasa**

Los límites de tasa se aplican por proyecto (no por clave de API) y se miden en tres dimensiones: Solicitudes por Minuto (RPM), Tokens por Minuto (TPM) y Solicitudes por Día (RPD).17 Estos límites varían significativamente según el modelo. Por ejemplo, Gemini 2.0 Flash tiene un límite alto de 10.000 RPM y 10M TPM, mientras que los modelos Gemma 3 y 3n tienen límites más conservadores de 30 RPM y 15K TPM.17 Las solicitudes en modo por lotes están sujetas a sus propios límites de tasa distintos.17

### **Estrategias para la Optimización de Costos**

* **Aprovechar los Niveles Gratuitos:** Utilizar ampliamente el uso gratuito de Google AI Studio y el nivel gratuito de la API de Gemini para todas las fases de desarrollo, pruebas y creación de prototipos.11  
* **Selección Estratégica de Modelos:** Elegir modelos basándose en los requisitos específicos de la tarea, considerando cuidadosamente el equilibrio entre la longitud del contexto, el tamaño de los parámetros, el rendimiento y la rentabilidad.37 Por ejemplo, modelos Flash para velocidad y costo, Pro para inteligencia, Gemma para necesidades ligeras.  
* **Ingeniería de Prompts para la Eficiencia:** Minimizar el uso de tokens elaborando prompts concisos y efectivos que obtengan la respuesta deseada sin verbosidad innecesaria.37  
* **Monitorear el Uso:** Monitorear activamente el uso de tokens para asegurar que las operaciones se mantengan dentro de los límites del nivel gratuito durante el desarrollo, y para gestionar los costos de manera efectiva en los niveles de pago.37

El enfoque de Google en cuanto a precios y acceso revela una estrategia dual sofisticada. Al ofrecer un AI Studio "completamente gratuito" y un generoso nivel gratuito de API 11, Google busca democratizar el acceso a sus modelos de IA, fomentando una adopción generalizada por parte de los desarrolladores y alentando la innovación. Este modelo "freemium" permite a los desarrolladores experimentar y construir sin una inversión inicial significativa. Sin embargo, la introducción de niveles de pago complejos con tarifas variables basadas en tokens, versiones de modelos y volumen de uso 11 representa un camino de monetización claro para el uso en producción a escala. Las suscripciones separadas "AI Pro/Ultra" orientadas al consumidor 13 indican una estrategia de monetización paralela y directa al consumidor, dirigida a usuarios finales que desean experiencias de IA premium. La política de datos en el nivel gratuito —donde los datos

*pueden ser utilizados para mejorar los productos de Google* 11— sirve como una forma de pago, permitiendo a Google refinar y mejorar continuamente sus modelos a través de datos de uso del mundo real. Esta estrategia multifacética posiciona a Google para capturar tanto el ecosistema de desarrolladores (a través de ofertas de API escalables) como el mercado de consumidores directos (a través de suscripciones premium). Para los desarrolladores, significa navegar por un panorama donde el desarrollo inicial es de bajo costo, pero la escalabilidad a producción requiere una planificación financiera y una optimización cuidadosas. Los límites de tasa y precios variables entre los diferentes modelos requieren una elección estratégica del modelo para la producción, equilibrando el rendimiento deseado, las limitaciones de costos y el volumen de uso anticipado. Los desarrolladores también deben ser muy conscientes de la política de uso de datos en el nivel gratuito, especialmente para aplicaciones que manejan información sensible, y considerar la actualización a niveles de pago para obtener garantías mejoradas de manejo de datos.

### **Tabla 4: Resumen de Precios de la API de Gemini para Llamada a Funciones**

Esta tabla proporciona una comparación clara y concisa de los costos asociados con el uso de diferentes modelos Gemini a través de la API, destacando específicamente los precios de entrada/salida y la disponibilidad del nivel gratuito.

| Nombre del Modelo | Disponibilidad del Nivel Gratuito (API) | Precio de Entrada (por 1M de tokens) | Precio de Salida (por 1M de tokens) | Precio de Caché de Contexto (por 1M de tokens/hora) | Datos Utilizados para Entrenamiento (Nivel Gratuito) | Ventana de Contexto de Entrada Máxima |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| Gemini 2.5 Pro | Sí (límites más bajos) | $1.25 (\<=200k), $2.50 (\>200k) | $10.00 (\<=200k), $15.00 (\>200k) | $0.31 (\<=200k) | Sí | 1.048.576 tokens |
| Gemini 2.5 Flash | Sí (límites más bajos) | $0.10 | $0.40 | $0.025 | Sí | 1.000.000 tokens |
| Gemini 2.0 Flash | Sí (límites más bajos) | $0.10 | $0.40 | $0.025 | Sí | 1.048.576 tokens |
| Gemma 3 & 3n | Sí (límites más bajos) | $0.075 | $0.30 | No disponible | Sí | Varía (32K-128K) |
| Gemini Embedding | Sí (límites más bajos) | $0.15 | N/A | N/A | Sí | N/A |

## **VII. Conclusión y Recomendaciones Estratégicas**

### **Fortalezas Sintetizadas de los Modelos de Google AI para la Llamada a Funciones**

Los modelos de Google AI, Gemini y Gemma, ofrecen un soporte robusto y sofisticado para la llamada a funciones, incluyendo la capacidad de manejar llamadas paralelas y composicionales. Esto es crucial para construir flujos de trabajo agenciales complejos y de múltiples pasos que interactúan con el mundo real.12 La comprensión multimodal inherente de los modelos (procesamiento de texto, imágenes, audio, video) y sus ventanas de contexto excepcionalmente grandes (hasta 1 millón de tokens para Gemini) proporcionan ventajas significativas. Estas características permiten a los modelos procesar entradas diversas y extensas, manteniendo una profunda memoria conversacional, vital para el uso efectivo de herramientas en escenarios complejos.4 La accesibilidad para los desarrolladores es notable, ya que el uso completamente gratuito de Google AI Studio para la experimentación y el generoso nivel gratuito ofrecido para la API de Gemini reducen significativamente la barrera de entrada, haciendo que estas capacidades avanzadas de IA sean accesibles para una amplia gama de desarrolladores para la creación de prototipos y el desarrollo inicial.11 Además, Google ofrece una gama estratégica de modelos (Gemini Pro para alta inteligencia, Gemini Flash para eficiencia en velocidad y costo, y Gemma para implementaciones ligeras y de código abierto). Esto permite a los desarrolladores optimizar su selección de modelos basándose en los requisitos específicos de inteligencia, velocidad y costo de su aplicación.5

### **Desafíos y Limitaciones Sintetizados**

A pesar de los sólidos benchmarks, la retroalimentación de la comunidad indica que la llamada a funciones en el mundo real puede ser impredecible e inconsistente. Esto incluye casos en los que los modelos no generan las llamadas a funciones esperadas, producen salidas inesperadas (por ejemplo, código markdown en lugar de llamadas a herramientas) o exhiben un rendimiento degradado en conversaciones largas y de múltiples turnos, especialmente con contextos grandes.14 En ciertas tareas complejas, particularmente de codificación, los modelos a veces pueden sobre-ingenierizar soluciones, alucinar cambios o desviarse de instrucciones precisas, lo que requiere una ingeniería de prompts y una validación cuidadosas.15 Finalmente, la política de utilizar datos del nivel gratuito para mejorar el modelo es una consideración para los desarrolladores que trabajan en aplicaciones sensibles a la privacidad.11

### **Recomendaciones para Desarrolladores y Empresas**

* **Comenzar con el Nivel Gratuito para Prototipos y Validación:** Aprovechar al máximo el uso gratuito de Google AI Studio y el nivel gratuito de la API de Gemini para el desarrollo inicial, las pruebas y la validación de la viabilidad de los conceptos de llamada a funciones y las integraciones de herramientas.11  
* **Implementar Capas de Orquestación Robustas:** Reconocer que el LLM es un orquestador inteligente, no un ejecutor. Diseñar la aplicación para manejar todo el flujo de trabajo de llamada a funciones de múltiples pasos, incluyendo el análisis sofisticado de las respuestas del LLM, la ejecución segura de herramientas externas, la gestión diligente del estado y el manejo integral de errores con lógica de reintento.18  
* **Realizar Pruebas Exhaustivas en Escenarios Realistas:** No depender únicamente de las puntuaciones de los benchmarks. Realizar pruebas extensas con conversaciones complejas de múltiples turnos, entradas diversas y contextos grandes para identificar y abordar de manera proactiva posibles inconsistencias, comportamientos inesperados del modelo o degradación del rendimiento antes de la implementación en producción.14  
* **Optimizar la Selección del Modelo para Tareas Específicas:** Elegir cuidadosamente el modelo Gemini (Pro para máxima inteligencia, Flash para velocidad/costo equilibrados) o Gemma (para necesidades ligeras y de código abierto) que mejor se alinee con los requisitos específicos de la aplicación, considerando factores como la longitud del contexto, la latencia y las implicaciones de costos.37  
* **Adherirse a las Mejores Prácticas para la Definición de Funciones:** Seguir las recomendaciones de Google para escribir declaraciones de funciones claras y detalladas, usar parámetros fuertemente tipados y utilizar eficazmente las instrucciones del sistema y los parámetros de FunctionCallingConfig (como mode y allowed\_function\_names) para guiar el comportamiento del modelo.1  
* **Monitoreo e Iteración Continuos:** Implementar un monitoreo robusto para las salidas del LLM y las interacciones de las herramientas en producción. Estar preparado para iterar continuamente en las instrucciones, refinar las definiciones de herramientas y adaptar la lógica de la aplicación para responder a las actualizaciones del modelo o abordar problemas emergentes.

La combinación de las significativas fortalezas de Google en modelos multimodales de contexto largo con llamada a funciones, junto con la retroalimentación franca de la comunidad sobre problemas de fiabilidad y consistencia, pinta un cuadro realista del panorama en rápida evolución de la IA agencial. El claro compromiso de Google de impulsar los límites de las capacidades agenciales es evidente en el desarrollo de sus modelos. Sin embargo, los desafíos reportados, particularmente con las interacciones de múltiples turnos y el seguimiento preciso de las instrucciones en escenarios complejos, sugieren que la visión de agentes de IA completamente autónomos y perfectamente fiables sigue siendo un trabajo en progreso. El entorno competitivo, con otros actores importantes como DeepSeek, Mistral y Kimi K2 también ofreciendo y mejorando sus capacidades de llamada a funciones (como se ve en los contextos de OpenRouter: 38), significa que Google debe priorizar continuamente las mejoras en la consistencia, robustez y adherencia a las instrucciones para mantener su ventaja competitiva y fomentar la confianza de los desarrolladores. Para los desarrolladores que se aventuran en el espacio de la IA agencial, este entorno dinámico significa que, si bien se están volviendo disponibles herramientas increíblemente potentes, una implementación exitosa requiere más que solo comprender las capacidades del LLM. Exige una profunda apreciación de sus limitaciones actuales, junto con prácticas sólidas de ingeniería de software. La estrategia de Google de proporcionar tanto modelos propietarios de vanguardia (Gemini) como modelos accesibles de código abierto (Gemma) ofrece una flexibilidad significativa, pero también requiere que los desarrolladores naveguen por diferentes niveles de control, transparencia y soporte comunitario. El futuro de las aplicaciones de IA dependerá cada vez más de la eficacia con la que los desarrolladores puedan cerrar la brecha entre la inteligencia del LLM y la fiabilidad en el mundo real a través de un diseño de sistema bien pensado.

#### **Fuentes citadas**

1. Function calling reference | Generative AI on Vertex AI \- Google Cloud, acceso: julio 20, 2025, [https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/function-calling](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/function-calling)  
2. Function Calling Guide: Google DeepMind Gemini 2.0 Flash, acceso: julio 20, 2025, [https://www.philschmid.de/gemini-function-calling](https://www.philschmid.de/gemini-function-calling)  
3. Function calling & tool use \- Gemini by Example, acceso: julio 20, 2025, [https://geminibyexample.com/021-tool-use-function-calling/](https://geminibyexample.com/021-tool-use-function-calling/)  
4. Gemini 2.5 Pro | Generative AI on Vertex AI \- Google Cloud, acceso: julio 20, 2025, [https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro)  
5. Gemini Flash \- Google DeepMind, acceso: julio 20, 2025, [https://deepmind.google/models/gemini/flash/](https://deepmind.google/models/gemini/flash/)  
6. Gemini 2.0 Flash | Generative AI on Vertex AI \- Google Cloud, acceso: julio 20, 2025, [https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash](https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash)  
7. Fine-Tuning Gemma 3 1B for Function Calling: A Step-by-Step Guide \- Medium, acceso: julio 20, 2025, [https://medium.com/@lucamassaron/fine-tuning-gemma-3-1b-for-function-calling-a-step-by-step-guide-66a613352f99](https://medium.com/@lucamassaron/fine-tuning-gemma-3-1b-for-function-calling-a-step-by-step-guide-66a613352f99)  
8. Google Gemma 3 Function Calling Example, acceso: julio 20, 2025, [https://www.philschmid.de/gemma-function-calling](https://www.philschmid.de/gemma-function-calling)  
9. Gemma 3 model overview | Google AI for Developers \- Gemini API, acceso: julio 20, 2025, [https://ai.google.dev/gemma/docs/core](https://ai.google.dev/gemma/docs/core)  
10. Introducing Gemma 3: The most capable model you can run on a single GPU or TPU, acceso: julio 20, 2025, [https://blog.google/technology/developers/gemma-3/](https://blog.google/technology/developers/gemma-3/)  
11. Gemini Developer API Pricing | Gemini API | Google AI for Developers, acceso: julio 20, 2025, [https://ai.google.dev/gemini-api/docs/pricing](https://ai.google.dev/gemini-api/docs/pricing)  
12. Function calling with the Gemini API | Google AI for Developers, acceso: julio 20, 2025, [https://ai.google.dev/gemini-api/docs/function-calling](https://ai.google.dev/gemini-api/docs/function-calling)  
13. Google AI Plans and Features, acceso: julio 20, 2025, [https://one.google.com/about/google-ai-plans/](https://one.google.com/about/google-ai-plans/)  
14. Very frustrating experience with Gemini 2.5 function calling performance, acceso: julio 20, 2025, [https://discuss.ai.google.dev/t/very-frustrating-experience-with-gemini-2-5-function-calling-performance/92814](https://discuss.ai.google.dev/t/very-frustrating-experience-with-gemini-2-5-function-calling-performance/92814)  
15. Gemini 2.5 Flash (05-20) Benchmark : r/LocalLLaMA \- Reddit, acceso: julio 20, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1krcdg5/gemini\_25\_flash\_0520\_benchmark/](https://www.reddit.com/r/LocalLLaMA/comments/1krcdg5/gemini_25_flash_0520_benchmark/)  
16. Problems With gemini-2.0-flash Tools : r/Bard \- Reddit, acceso: julio 20, 2025, [https://www.reddit.com/r/Bard/comments/1isf7p5/problems\_with\_gemini20flash\_tools/](https://www.reddit.com/r/Bard/comments/1isf7p5/problems_with_gemini20flash_tools/)  
17. Rate limits | Gemini API | Google AI for Developers, acceso: julio 20, 2025, [https://ai.google.dev/gemini-api/docs/rate-limits](https://ai.google.dev/gemini-api/docs/rate-limits)  
18. Tool & Function Calling | Use Tools with OpenRouter | OpenRouter ..., acceso: julio 20, 2025, [https://openrouter.ai/docs/features/tool-calling](https://openrouter.ai/docs/features/tool-calling)  
19. google/gemma-3-1b-it \- Hugging Face, acceso: julio 20, 2025, [https://huggingface.co/google/gemma-3-1b-it](https://huggingface.co/google/gemma-3-1b-it)  
20. Gemini 2.5: Our most intelligent AI model \- Google Blog, acceso: julio 20, 2025, [https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)  
21. Gemma 3 – Vertex AI \- Google Cloud console, acceso: julio 20, 2025, [https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3?hl=ko](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3?hl=ko)  
22. Function calling with Gemma | Google AI for Developers \- Gemini API, acceso: julio 20, 2025, [https://ai.google.dev/gemma/docs/capabilities/function-calling](https://ai.google.dev/gemma/docs/capabilities/function-calling)  
23. Start building with Gemini 2.0 Flash and Flash-Lite \- Google Developers Blog, acceso: julio 20, 2025, [https://developers.googleblog.com/en/start-building-with-the-gemini-2-0-flash-family/](https://developers.googleblog.com/en/start-building-with-the-gemini-2-0-flash-family/)  
24. Models \- Google DeepMind, acceso: julio 20, 2025, [https://deepmind.google/models/](https://deepmind.google/models/)  
25. Gemma 3n 2B (free) \- API, Providers, Stats \- OpenRouter, acceso: julio 20, 2025, [https://openrouter.ai/google/gemma-3n-e2b-it:free](https://openrouter.ai/google/gemma-3n-e2b-it:free)  
26. Google models | Generative AI on Vertex AI, acceso: julio 20, 2025, [https://cloud.google.com/vertex-ai/generative-ai/docs/models](https://cloud.google.com/vertex-ai/generative-ai/docs/models)  
27. OpenRouter Model Price Comparison, acceso: julio 20, 2025, [https://compare-openrouter-models.pages.dev/](https://compare-openrouter-models.pages.dev/)  
28. Gemini 2.5: Updates to our family of thinking models \- Google Developers Blog, acceso: julio 20, 2025, [https://developers.googleblog.com/en/gemini-2-5-thinking-model-updates/](https://developers.googleblog.com/en/gemini-2-5-thinking-model-updates/)  
29. Is DeepSeek R1-0528 function call chat template supported BFCL multi turn benchmark?, acceso: julio 20, 2025, [https://discuss.vllm.ai/t/is-deepseek-r1-0528-function-call-chat-template-supported-bfcl-multi-turn-benchmark/1039](https://discuss.vllm.ai/t/is-deepseek-r1-0528-function-call-chat-template-supported-bfcl-multi-turn-benchmark/1039)  
30. Gemini 2.5 pro context window issues : r/GoogleGeminiAI \- Reddit, acceso: julio 20, 2025, [https://www.reddit.com/r/GoogleGeminiAI/comments/1lzjr5v/gemini\_25\_pro\_context\_window\_issues/](https://www.reddit.com/r/GoogleGeminiAI/comments/1lzjr5v/gemini_25_pro_context_window_issues/)  
31. Gemini Flash 2.5 cannot use the word “browsing” : r/singularity \- Reddit, acceso: julio 20, 2025, [https://www.reddit.com/r/singularity/comments/1m4b21n/gemini\_flash\_25\_cannot\_use\_the\_word\_browsing/](https://www.reddit.com/r/singularity/comments/1m4b21n/gemini_flash_25_cannot_use_the_word_browsing/)  
32. Invalid Model for gemini-2.5-pro-latest \- Bug Reports \- Cursor \- Community Forum, acceso: julio 20, 2025, [https://forum.cursor.com/t/invalid-model-for-gemini-2-5-pro-latest/118892](https://forum.cursor.com/t/invalid-model-for-gemini-2-5-pro-latest/118892)  
33. Gemini 2.5 Flash | Hacker News, acceso: julio 20, 2025, [https://news.ycombinator.com/item?id=43720845](https://news.ycombinator.com/item?id=43720845)  
34. Introduction to function calling | Generative AI on Vertex AI \- Google Cloud, acceso: julio 20, 2025, [https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling)  
35. Gemini API Costs: Factors, Benefits, and How to Use It? \- Talentelgia Technologies, acceso: julio 20, 2025, [https://www.talentelgia.com/blog/gemini-api-cost/](https://www.talentelgia.com/blog/gemini-api-cost/)  
36. Google AI Pro & Ultra — get access to Gemini 2.5 Pro & more, acceso: julio 20, 2025, [https://gemini.google/subscriptions/](https://gemini.google/subscriptions/)  
37. How to Use LLMs for Free ? \- Apidog, acceso: julio 20, 2025, [https://apidog.com/blog/use-llms-for-free/](https://apidog.com/blog/use-llms-for-free/)  
38. Cypher Alpha: What's the Free Mysterious OpenRouter API? \- Apidog, acceso: julio 20, 2025, [https://apidog.com/blog/cypher-alpha/](https://apidog.com/blog/cypher-alpha/)  
39. Comparison: TensorZero vs. OpenRouter, acceso: julio 20, 2025, [https://www.tensorzero.com/docs/comparison/openrouter/](https://www.tensorzero.com/docs/comparison/openrouter/)  
40. Function Calling \- DeepSeek API Docs, acceso: julio 20, 2025, [https://api-docs.deepseek.com/guides/function\_calling](https://api-docs.deepseek.com/guides/function_calling)  
41. Models: 'free' \- OpenRouter, acceso: julio 20, 2025, [https://openrouter.ai/models/?q=free](https://openrouter.ai/models/?q=free)  
42. Models: 'morph-v3-fast' \- OpenRouter, acceso: julio 20, 2025, [https://openrouter.ai/)](https://openrouter.ai/\))  
43. Kimi K2 \- API, Providers, Stats \- OpenRouter, acceso: julio 20, 2025, [https://openrouter.ai/moonshotai/kimi-k2](https://openrouter.ai/moonshotai/kimi-k2)  
44. Mistral Small 3.2 24B (free) \- API, Providers, Stats \- OpenRouter, acceso: julio 20, 2025, [https://openrouter.ai/mistralai/mistral-small-3.2-24b-instruct:free](https://openrouter.ai/mistralai/mistral-small-3.2-24b-instruct:free)  
45. Devstral Small 1.1 \- API, Providers, Stats \- OpenRouter, acceso: julio 20, 2025, [https://openrouter.ai/mistralai/devstral-small](https://openrouter.ai/mistralai/devstral-small)  
46. DeepSeek's New R1–0528: Performance Analysis and Benchmark Comparisons \- Medium, acceso: julio 20, 2025, [https://medium.com/@leucopsis/deepseeks-new-r1-0528-performance-analysis-and-benchmark-comparisons-6440eac858d6](https://medium.com/@leucopsis/deepseeks-new-r1-0528-performance-analysis-and-benchmark-comparisons-6440eac858d6)  
47. moonshotai/Kimi-K2-Instruct (and Kimi-K2-Base) : r/LocalLLaMA \- Reddit, acceso: julio 20, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1lx8xdm/moonshotaikimik2instruct\_and\_kimik2base/](https://www.reddit.com/r/LocalLLaMA/comments/1lx8xdm/moonshotaikimik2instruct_and_kimik2base/)  
48. The new DeepSeek R1 0528 now supports native tool calling on OpenRouter\! \- Reddit, acceso: julio 20, 2025, [https://www.reddit.com/r/DeepSeek/comments/1l84cx3/the\_new\_deepseek\_r1\_0528\_now\_supports\_native\_tool/](https://www.reddit.com/r/DeepSeek/comments/1l84cx3/the_new_deepseek_r1_0528_now_supports_native_tool/)  
49. mistralai/Mistral-Small-3.2-24B-Instruct-2506 · Hugging Face : r/LocalLLaMA \- Reddit, acceso: julio 20, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1lg7vuc/mistralaimistralsmall3224binstruct2506\_hugging/](https://www.reddit.com/r/LocalLLaMA/comments/1lg7vuc/mistralaimistralsmall3224binstruct2506_hugging/)  
50. mistralai/Devstral-Small-2507 : r/LocalLLaMA \- Reddit, acceso: julio 20, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/1lwe5y8/mistralaidevstralsmall2507/](https://www.reddit.com/r/LocalLLaMA/comments/1lwe5y8/mistralaidevstralsmall2507/)  
51. Upgrading agentic coding capabilities with the new Devstral models \- Mistral AI, acceso: julio 20, 2025, [https://mistral.ai/news/devstral-2507](https://mistral.ai/news/devstral-2507)  
52. New Stealth Model: "Cypher Alpha" \- OpenRouter, acceso: julio 20, 2025, [https://openrouter.ai/announcements/new-stealth-model-cypher-alpha](https://openrouter.ai/announcements/new-stealth-model-cypher-alpha)  
53. Cypher Alpha \- API, Providers, Stats \- OpenRouter, acceso: julio 20, 2025, [https://openrouter.ai/openrouter/cypher-alpha:free](https://openrouter.ai/openrouter/cypher-alpha:free)  
54. Models | OpenRouter, acceso: julio 20, 2025, [https://openrouter.ai/models?q=free\&supported\_parameters=tools](https://openrouter.ai/models?q=free&supported_parameters=tools)  
55. Open Source Function Calling LLMs : r/LocalLLaMA \- Reddit, acceso: julio 20, 2025, [https://www.reddit.com/r/LocalLLaMA/comments/190f62z/open\_source\_function\_calling\_llms/](https://www.reddit.com/r/LocalLLaMA/comments/190f62z/open_source_function_calling_llms/)  
56. OpenRouter \- Haystack Documentation \- Deepset, acceso: julio 20, 2025, [https://docs.haystack.deepset.ai/v2.1/reference/integrations-openrouter](https://docs.haystack.deepset.ai/v2.1/reference/integrations-openrouter)  
57. Model Comparison \- OpenRouter, acceso: julio 20, 2025, [https://openrouter.ai/compare/openai/o3-mini](https://openrouter.ai/compare/openai/o3-mini)  
58. Model Comparison \- OpenRouter, acceso: julio 20, 2025, [https://openrouter.ai/compare/openai/chatgpt-4o-latest](https://openrouter.ai/compare/openai/chatgpt-4o-latest)